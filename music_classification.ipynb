{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "CNN algorithms recognize patterns in spatial data, which works best with images. So we will be converting the original audio data into spectrograms which are graphs that visually represent the change in frequency over time.<br>\n",
    "We are starting with 1000 wav files for our data. I will convert these into mel-spectrogrmas. we chose mel-spectrograms specifically because they measure the mel scale instead of frequency along the y-axis. Also changing the color of the points based off the decibal scale not the amplitude of the wave. These spectrograms focus more on what humans will actually here making it more ideal for genre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# generate genres_img folder for spectrograms\n",
    "\n",
    "main_dir = \"data\"\n",
    "genres_dir = os.path.join(main_dir, \"genres_img\")\n",
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n",
    "    print(f\"Created directory: {main_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {main_dir}\")\n",
    "\n",
    "if not os.path.exists(genres_dir):\n",
    "    os.makedirs(genres_dir)\n",
    "    print(f\"Created directory: {genres_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {genres_dir}\")\n",
    "\n",
    "for genre in genres:\n",
    "    genre_dir = os.path.join(genres_dir, genre)\n",
    "    if not os.path.exists(genre_dir):\n",
    "        os.makedirs(genre_dir)\n",
    "        print(f\"Created directory: {genre_dir}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {genre_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert the wav file to mel-spectrogram\n",
    "def save_mel_spectrogram(wav_path, output_image_path, sr=22050, n_mels=128):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "\n",
    "    # Generate Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    \n",
    "    # Convert to decibels for better visualization\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "\n",
    "    # Remove axes for a clean image\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save as an image\n",
    "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "PATH_MP3 = \"./data/genres_original/\"\n",
    "PATH_IMG = \"./data/genres_img/\"\n",
    "\n",
    "# script to convert all wav to mel-spectrogram\n",
    "for genre in os.listdir(PATH_MP3):\n",
    "    for music in os.listdir(PATH_MP3+genre):\n",
    "        save_mel_spectrogram(f\"{PATH_MP3}{genre}/{music}\", f\"{PATH_IMG}{genre}/{music[:-3]}png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first model made had 47% accuracy after the test data\n",
    "\n",
    "increased epoch to 50, and increased neuron connection to 128 -> 256\n",
    "11% accuracy\n",
    "\n",
    "decreased epoch to 50 -> 20, changed droprate 0.5 -> 0.2, neuron connection 256 -> 128\n",
    "\n",
    "changes doubled filter size at each layer from 32 -> 64 and so on\n",
    "added BatchNormalization() after each layer\n",
    "increaed neuron layers at end from 128 -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 10 classes.\n",
      "Found 200 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 184s 7s/step - loss: 56.9789 - accuracy: 0.1988 - val_loss: 76.2724 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 184s 7s/step - loss: 3.0555 - accuracy: 0.1600 - val_loss: 283.6599 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.7787 - accuracy: 0.1375 - val_loss: 343.0875 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.6186 - accuracy: 0.1488 - val_loss: 271.4120 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.3669 - accuracy: 0.1225 - val_loss: 187.9354 - val_accuracy: 0.0950\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.4716 - accuracy: 0.1050 - val_loss: 160.5865 - val_accuracy: 0.0950\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.4015 - accuracy: 0.1150 - val_loss: 150.4043 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2894 - accuracy: 0.0925 - val_loss: 112.8279 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2627 - accuracy: 0.1013 - val_loss: 91.6263 - val_accuracy: 0.1200\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2522 - accuracy: 0.1175 - val_loss: 80.9143 - val_accuracy: 0.1200\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 182s 7s/step - loss: 2.2712 - accuracy: 0.1025 - val_loss: 93.1097 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 182s 7s/step - loss: 2.2846 - accuracy: 0.0988 - val_loss: 81.7787 - val_accuracy: 0.1150\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2788 - accuracy: 0.1138 - val_loss: 66.7065 - val_accuracy: 0.1350\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2702 - accuracy: 0.1025 - val_loss: 57.3726 - val_accuracy: 0.1550\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 182s 7s/step - loss: 2.3417 - accuracy: 0.1050 - val_loss: 25.6615 - val_accuracy: 0.1500\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2668 - accuracy: 0.1050 - val_loss: 16.3948 - val_accuracy: 0.1350\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2535 - accuracy: 0.1225 - val_loss: 9.5235 - val_accuracy: 0.1000\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.3224 - accuracy: 0.1150 - val_loss: 3.0080 - val_accuracy: 0.1150\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.3179 - accuracy: 0.1138 - val_loss: 2.5131 - val_accuracy: 0.1400\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2692 - accuracy: 0.1075 - val_loss: 3.6505 - val_accuracy: 0.0950\n",
      "7/7 [==============================] - 9s 1s/step - loss: 3.6505 - accuracy: 0.0950\n",
      "Validation Accuracy: 0.0950\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Set parameters\n",
    "img_height = 308  # Resize all images to 128x128\n",
    "img_width = 775\n",
    "batch_size = 32  # Process 32 images at a time\n",
    "data_dir = \"./data/genres_img/\"  # Path to dataset folder\n",
    "\n",
    "# Create data generators\n",
    "train_datagen =  ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),  \n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    "    subset='training'  \n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Uses 20% for validation\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    # creates 32 filters small 3x3 grids that slide over the image looking for patters\n",
    "    # relu is Rectified Linear Unit\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    BatchNormalization(),\n",
    "    # reduces the size of the images taking the max values it found in each region\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    # adds neuron layers together to combine extracted features\n",
    "    Dense(128, activation='relu'),\n",
    "    # randomly removes 50% of the neurons to prevent overfitting\n",
    "    Dropout(0.2),\n",
    "    # assigns probabilities to each of the 10 genres\n",
    "    Dense(10, activation='softmax')  # 10 output classes (one for each genre)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20  # Number of times model sees the dataset\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,  # Training data\n",
    "    validation_data=val_generator,  # Validation data\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "model.save(\"music_genre_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('music_genre_cnn.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
