{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "CNN algorithms recognize patterns in spatial data, which works best with images. So we will be converting the original audio data into spectrograms which are graphs that visually represent the change in frequency over time.<br>\n",
    "We are starting with 1000 wav files for our data. I will convert these into mel-spectrogrmas. we chose mel-spectrograms specifically because they measure the mel scale instead of frequency along the y-axis. Also changing the color of the points based off the decibal scale not the amplitude of the wave. These spectrograms focus more on what humans will actually here making it more ideal for genre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to convert a 3-second audio segment to a mel-spectrogram\n",
    "def save_mel_spectrogram(y, sr, output_image_path, n_mels=128):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Paths\n",
    "PATH_MP3 = \"./data/genres_original/\"\n",
    "PATH_IMG = \"./data/genres_img/\"\n",
    "sr = 22050 \n",
    "skip = []\n",
    "\n",
    "os.makedirs(PATH_IMG, exist_ok=True)\n",
    "\n",
    "# Convert all WAV files to mel spectrograms (split into 3-second segments)\n",
    "for genre in os.listdir(PATH_MP3):\n",
    "    if genre not in skip:\n",
    "        genre_path = os.path.join(PATH_MP3, genre)\n",
    "        genre_img_path = os.path.join(PATH_IMG, genre)\n",
    "        os.makedirs(genre_img_path, exist_ok=True)  \n",
    "        \n",
    "        for music in os.listdir(genre_path):\n",
    "            wav_path = os.path.join(genre_path, music)\n",
    "            y, sr = librosa.load(wav_path, sr=sr)\n",
    "\n",
    "            # Total duration of the file (should be ~30 sec)\n",
    "            total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "            segment_length = sr * 3  \n",
    "\n",
    "            for i in range(10):  \n",
    "                start_sample = i * segment_length\n",
    "                end_sample = start_sample + segment_length\n",
    "                if end_sample > len(y):  \n",
    "                    break\n",
    "                segment = y[start_sample:end_sample]\n",
    "                output_img_path = os.path.join(genre_img_path, f\"{music[:-4]}_{i}.png\") \n",
    "                save_mel_spectrogram(segment, sr, output_img_path)\n",
    "\n",
    "        print(f\"Finished: {genre}\")\n",
    "    else:\n",
    "        print(f\"Skipped: {genre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model using this will have ~84% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 10 classes.\n",
      "Found 200 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 184s 7s/step - loss: 56.9789 - accuracy: 0.1988 - val_loss: 76.2724 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 184s 7s/step - loss: 3.0555 - accuracy: 0.1600 - val_loss: 283.6599 - val_accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.7787 - accuracy: 0.1375 - val_loss: 343.0875 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.6186 - accuracy: 0.1488 - val_loss: 271.4120 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.3669 - accuracy: 0.1225 - val_loss: 187.9354 - val_accuracy: 0.0950\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.4716 - accuracy: 0.1050 - val_loss: 160.5865 - val_accuracy: 0.0950\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.4015 - accuracy: 0.1150 - val_loss: 150.4043 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2894 - accuracy: 0.0925 - val_loss: 112.8279 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2627 - accuracy: 0.1013 - val_loss: 91.6263 - val_accuracy: 0.1200\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2522 - accuracy: 0.1175 - val_loss: 80.9143 - val_accuracy: 0.1200\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 182s 7s/step - loss: 2.2712 - accuracy: 0.1025 - val_loss: 93.1097 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 182s 7s/step - loss: 2.2846 - accuracy: 0.0988 - val_loss: 81.7787 - val_accuracy: 0.1150\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2788 - accuracy: 0.1138 - val_loss: 66.7065 - val_accuracy: 0.1350\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2702 - accuracy: 0.1025 - val_loss: 57.3726 - val_accuracy: 0.1550\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 182s 7s/step - loss: 2.3417 - accuracy: 0.1050 - val_loss: 25.6615 - val_accuracy: 0.1500\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2668 - accuracy: 0.1050 - val_loss: 16.3948 - val_accuracy: 0.1350\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2535 - accuracy: 0.1225 - val_loss: 9.5235 - val_accuracy: 0.1000\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.3224 - accuracy: 0.1150 - val_loss: 3.0080 - val_accuracy: 0.1150\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.3179 - accuracy: 0.1138 - val_loss: 2.5131 - val_accuracy: 0.1400\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 183s 7s/step - loss: 2.2692 - accuracy: 0.1075 - val_loss: 3.6505 - val_accuracy: 0.0950\n",
      "7/7 [==============================] - 9s 1s/step - loss: 3.6505 - accuracy: 0.0950\n",
      "Validation Accuracy: 0.0950\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class MusicGenreCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MusicGenreCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(512 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(genre_labels)  # Number of music genres\n",
    "model = MusicGenreCNN(num_classes).to(device)\n",
    "\n",
    "# Define loss function (CrossEntropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (Adam for better convergence)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20 # Number of training iterations\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"music_genre_cnn.pth\")\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
